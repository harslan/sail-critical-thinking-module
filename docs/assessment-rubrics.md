# Assessment Rubrics

Standardized grading criteria for the SAIL Critical Thinking Module.

---

## Grading Philosophy

**Core Principle:** Grade on *authenticity and specificity*, not polish or eloquence.

A messy, honest reflection that engages with real AI interactions is worth more than a beautifully written but generic response that could apply to any student.

### What We're Assessing
- Genuine critical engagement with AI
- Self-awareness about limitations and learning
- Specific, actionable professional judgment
- Intellectual honesty

### What We're NOT Assessing
- Writing quality (beyond basic clarity)
- Technical sophistication of AI use
- "Correct" answers about AI ethics
- Volume or length of responses

---

## Phase 1: Foundation Task

### Rubric (20% of module grade)

| Criterion | Excellent (4) | Proficient (3) | Developing (2) | Beginning (1) |
|-----------|---------------|----------------|----------------|---------------|
| **Task Completion** | Thorough, good-faith effort addressing all components | Addresses most components adequately | Partial completion, some gaps | Minimal effort or major gaps |
| **Honest Reflection** | Candidly identifies specific uncertainties and limitations | Notes some uncertainties | Vague or generic reflection | No meaningful reflection |

**Note:** This phase is about establishing baseline, not demonstrating mastery. Students should feel safe acknowledging what they don't know.

---

## Phase 2: AI Integration & Critical Evaluation

### Part A: Interaction Documentation (15% of module grade)

| Criterion | Excellent (4) | Proficient (3) | Developing (2) | Beginning (1) |
|-----------|---------------|----------------|----------------|---------------|
| **Authenticity** | Clearly real interactions with natural flow, follow-ups, corrections | Appears genuine with some working process visible | Possibly staged; too clean or linear | Obviously fabricated or missing |
| **Completeness** | Full conversation arc: initial prompt → AI response → follow-up → refinement | Most of conversation documented | Partial documentation, key moments missing | Minimal or cherry-picked snippets |
| **Variety** | Shows multiple interaction strategies (questioning, redirecting, challenging) | Shows some strategic variation | Mostly single approach (accept/reject) | No evidence of strategic interaction |

**Red Flags:**
- Screenshots with no timestamps
- Conversations that read like scripted dialogue
- Perfect prompts with no iteration
- No evidence of pushback or correction

### Part B: Critical Reflection (25% of module grade)

| Criterion | Excellent (4) | Proficient (3) | Developing (2) | Beginning (1) |
|-----------|---------------|----------------|----------------|---------------|
| **Specificity** | Cites specific AI outputs with concrete examples; "AI said X but I knew Y because..." | References specific moments but less detailed | General observations without specific examples | Vague statements that could apply to anyone |
| **Critical Depth** | Identifies nuanced errors (context-missing, subtly wrong, technically correct but misleading) | Identifies clear errors or limitations | Notes only obvious errors | No meaningful critique; uncritical acceptance |
| **Self-Awareness** | Reflects on own learning: what they now see differently, how Phase 1 informed evaluation | Some reflection on personal growth | Minimal self-reflection | No evidence of metacognition |
| **Phase 1 Connection** | Explicitly connects Phase 1 work to Phase 2 evaluation capability | Some connection to prior work | Weak connection | Treats phases as unrelated |

**Exemplary Response Characteristics:**
- "In Phase 1, I wasn't sure about X. AI confidently stated Y, but because I'd struggled with it, I double-checked and found..."
- "AI missed the context that [specific domain knowledge]. I only caught this because..."
- "I initially accepted AI's framing, but then realized it assumed [unstated assumption]..."

**Weak Response Characteristics:**
- "AI was helpful but I had to check some things"
- "AI made a few mistakes that I corrected"
- Generic praise or criticism without specifics

---

## Phase 3: Personal AI Use Framework

### Framework Quality (20% of module grade)

| Criterion | Excellent (4) | Proficient (3) | Developing (2) | Beginning (1) |
|-----------|---------------|----------------|----------------|---------------|
| **Specificity** | Concrete, actionable guidelines tied to their professional context | Mostly specific with some general statements | Mix of specific and generic | Vague platitudes; could apply to anyone |
| **Completeness** | Addresses when to use, when not to use, verification process, conflict resolution, communication | Covers most areas adequately | Some areas underdeveloped | Major gaps in coverage |
| **Realism** | Acknowledges tradeoffs and constraints; practical for actual work | Generally realistic | Some unrealistic elements | Idealistic without practical grounding |
| **Principled** | Clear reasoning behind choices; connects to professional values | Some reasoning provided | Choices stated without rationale | Arbitrary or contradictory |

**Strong Framework Elements:**
- "I will use AI for [specific task] because [reason], but I will always verify [specific element] by [specific method]"
- "I will NOT use AI for [specific situation] because [professional/ethical reason]"
- "When AI and my judgment conflict, my process is: 1) [step], 2) [step], 3) [step]"
- "I will explain my AI use to [stakeholder] by [specific approach]"

**Weak Framework Elements:**
- "I will use AI responsibly"
- "I will always double-check AI's work"
- "I will be ethical"
- Lists copied from AI ethics articles

### Peer Review Quality (5% of module grade)

| Criterion | Excellent (4) | Proficient (3) | Developing (2) | Beginning (1) |
|-----------|---------------|----------------|----------------|---------------|
| **Constructiveness** | Specific strengths identified; actionable improvement suggestions | Notes strengths and offers suggestions | Generic feedback | Dismissive or unhelpful |
| **Thoughtfulness** | Engages seriously with peer's framework; shows careful reading | Shows engagement | Superficial engagement | Clearly didn't read carefully |
| **Question Quality** | Asks probing question that could genuinely improve the framework | Asks relevant question | Question is generic | No question or irrelevant question |

---

## Overall Module Grade Calculation

| Component | Weight |
|-----------|--------|
| Phase 1: Task + Reflection | 20% |
| Phase 2: Interaction Documentation | 15% |
| Phase 2: Critical Reflection | 25% |
| Phase 3: Framework | 20% |
| Phase 3: Peer Review | 5% |
| **Completion/Participation** | 15% |

### Credential Thresholds

| Credential | Minimum Score |
|------------|---------------|
| Bronze | Complete all components (any score) |
| Silver | Average 3.0+ on Phase 2 Reflection AND Phase 3 Framework |
| Gold | Silver + peer review rated 4 OR contribution to case library |

---

## Calibration Notes for Instructors

### Common Grading Pitfalls

1. **Over-rewarding polish:** A well-written but generic reflection shouldn't outscore a rough but specific one
2. **Penalizing AI criticism:** Students who note AI worked well aren't being uncritical — if they can explain *why* it worked, that's sophisticated
3. **Expecting expertise:** Students are learning; early-career-level judgment is the goal, not expert-level
4. **Uniformity bias:** Frameworks should differ based on discipline and personal values; variation is expected

### Anchor Examples

*To be populated with anonymized examples from pilot implementations*

---

## Accommodation Considerations

- Extended time for reflection writing does not compromise assessment validity
- Oral reflection option for students with writing accommodations
- Interaction documentation can be screen recordings if screenshots are difficult
- Peer review can be conducted in discussion rather than writing if needed

---

*Questions about rubric application? Contact harslan@suffolk.edu*
