# Implementation Guide

A step-by-step guide for faculty adopting the SAIL Critical Thinking Module in their courses.

---

## Overview

This module is designed to be **low-lift for faculty** while being **high-impact for students**. You don't need to become an AI expert — you just need to provide a discipline-relevant task and facilitate the reflection process.

## Time Commitment

| Your Role | Time Required |
|-----------|---------------|
| Initial setup | 1-2 hours (one-time) |
| Per cohort facilitation | 30-60 minutes per phase |
| Grading reflections | ~5 minutes per student |

## Before You Begin

### 1. Choose Your Domain-Specific Task

The module works with any task that:
- Requires analysis, synthesis, or judgment (not just recall)
- Can be meaningfully attempted both with and without AI
- Has enough complexity that AI assistance is genuinely useful
- Allows for multiple valid approaches

**Good examples:**
- Analyze a case study and recommend a strategy
- Evaluate a marketing campaign and suggest improvements
- Assess financial statements and identify concerns
- Draft a professional communication for a complex situation

**Less effective:**
- Simple calculations or lookups
- Tasks with single correct answers
- Pure memorization exercises

### 2. Define "Success" for Your Context

What does good critical thinking look like in your discipline? Consider:
- What errors would AI likely make in your domain?
- What context would AI miss that an expert would catch?
- What judgment calls require human values or priorities?

Document these — they'll inform your grading rubric.

---

## Phase-by-Phase Implementation

### Phase 1: Foundation (Think Without the Crutch)

**Duration:** 1 class session or 1 week asynchronous

**Student Task:**
Complete your domain-specific task using only their own knowledge and reasoning. No AI tools permitted.

**Your Role:**
- Provide clear task instructions
- Enforce the no-AI boundary (honor system or proctored)
- Collect submissions for comparison in Phase 2

**Canvas Setup:**
- Assignment with submission
- Due date before Phase 2 begins
- Consider: timed submission window to limit AI temptation

**What Students Submit:**
- Their completed task
- Brief reflection: "What was hardest? What were you unsure about?"

---

### Phase 2: Integration (Use AI, But Question Everything)

**Duration:** 1-2 class sessions or 1-2 weeks asynchronous

**Student Task:**
Repeat or extend the same task with full AI access. Document the process and critically evaluate AI contributions.

**Your Role:**
- Provide AI interaction documentation template (see `/canvas/assignment-prompts.md`)
- Review submissions for authentic critical engagement
- Facilitate discussion comparing Phase 1 vs Phase 2 work

**Canvas Setup:**
- Assignment with multiple submission components:
  1. Revised/extended task output
  2. AI interaction log (screenshots or exports)
  3. Critical reflection using provided prompts

**What Students Submit:**
1. **The work product** — Their AI-augmented output
2. **Interaction documentation** — Screenshots showing:
   - What they asked AI
   - What AI provided
   - Their follow-up questions or corrections
3. **Critical reflection** answering:
   - What did AI get right that you missed in Phase 1?
   - What did AI get wrong or miss entirely?
   - What did you have to correct, override, or supplement?
   - How did your Phase 1 foundation help you evaluate AI output?

---

### Phase 3: Leadership (Own Your Human-AI Partnership)

**Duration:** 1 class session or 1 week asynchronous

**Student Task:**
Develop a personal framework for AI use in their professional context, then defend it to peers.

**Your Role:**
- Provide framework template (see `/canvas/assignment-prompts.md`)
- Facilitate peer review process
- Assess framework quality using rubric

**Canvas Setup:**
- Assignment: Personal AI Use Framework
- Peer review assignment (Canvas built-in or discussion board)
- Optional: In-class presentation or discussion

**What Students Submit:**
1. **Personal AI Use Framework** addressing:
   - When will I use AI in my professional work?
   - When will I deliberately NOT use AI?
   - How will I verify AI outputs in my domain?
   - What's my process when AI and my judgment conflict?
   - How will I explain my AI use to colleagues/clients/supervisors?

2. **Peer Review** of one classmate's framework:
   - What's strongest about their approach?
   - What gap or risk do you see?
   - One question you'd want them to address

---

## Grading Approach

### What to Grade

| Component | Weight | What to Look For |
|-----------|--------|------------------|
| Phase 1 completion | 20% | Good-faith effort, honest reflection on uncertainty |
| Phase 2 documentation | 30% | Authentic interaction logs, specific examples |
| Phase 2 critical reflection | 25% | Genuine critique, not just praise for AI |
| Phase 3 framework | 20% | Thoughtful, specific, actionable guidelines |
| Peer review quality | 5% | Constructive, substantive feedback |

### Grading Philosophy

**Grade on specificity, not polish.**

A messy, honest reflection that says "AI suggested X but I knew from class that Y, so I pushed back" is worth more than an eloquent but generic reflection that could apply to any student in any course.

**Red flags for inauthentic work:**
- Generic observations that don't reference specific AI outputs
- Reflection language that sounds AI-generated (overly formal, hedge-heavy)
- Interaction logs that seem staged rather than working process
- Framework that reads like a policy document rather than personal commitment

See `/docs/assessment-rubrics.md` for detailed grading criteria.

---

## Customization Options

### Minimal Customization (Fastest)
Use the generic prompts in `/canvas/assignment-prompts.md` as-is. Works for any discipline.

### Moderate Customization (Recommended)
Replace the generic task with a discipline-specific case or assignment from your existing course. Keep the reflection prompts as-is.

### Full Customization
Adapt all prompts, add discipline-specific AI error examples, create custom rubric criteria. See `/examples` for models.

---

## Common Questions

**Q: What if students don't have AI access?**
A: Claude, ChatGPT, and other tools are free to use. Ensure students know this. If institutional barriers exist, contact your instructional designer.

**Q: How do I know if interaction logs are real?**
A: Look for messiness — real conversations have typos, follow-ups, and tangents. Staged logs are suspiciously clean. Timestamps help too.

**Q: What if students' Phase 1 work is terrible?**
A: That's actually useful data. Phase 2 reflection should acknowledge "I didn't know X, and AI helped me see that." Honesty matters more than competence.

**Q: Can I use this for a graded major assignment?**
A: Yes, but consider making the reflection components the bulk of the grade, not the task output itself. This keeps the focus on critical thinking, not AI-augmented performance.

---

## Getting Help

- **Technical issues:** Contact Suffolk instructional design
- **Pedagogical questions:** Reach out to the SAIL Collaborative
- **Share your adaptation:** Submit to `/examples` via pull request or email

---

*Questions? Contact harslan@suffolk.edu*
